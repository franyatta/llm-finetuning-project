# Model configuration
model_name: 'distilgpt2'
dataset_name: 'wikitext'
dataset_config: 'wikitext-2-raw-v1'
text_column: 'text'

# Training configuration
output_dir: './results_wikitext'
num_epochs: 3        # Kept: Wikipedia articles need more training
batch_size: 4        # Decreased: longer sequences need more memory
learning_rate: 1e-5  # Decreased: more stable for diverse content
max_length: 256      # Increased: capture more article context